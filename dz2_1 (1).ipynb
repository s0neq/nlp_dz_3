{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpRLBbSKU7vi"
   },
   "source": [
    "### дз2 Генералова Софья"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "LgaYR6LLdkjp",
    "outputId": "e5e13b73-7992-456f-cd0f-b1455a7c595a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2.post1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.16.0)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nw9zNs6dlVm"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugAILjetU7vj"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4F8y2YzU7vo"
   },
   "source": [
    "найти или самим написать русский и английский тексты (каждый от ста слов), в которых  будут какие-то трудные или неоднозначные для POS теггинга моменты и разметить их в ручную (а потом объяснить, какие моменты вы тут считаете трудными для автоматического посттеггинга и почему) – с помощью этих текстов мы будем оценивать качество работы наших теггоров. В текстах размечаем только части речи, ничего больше!\n",
    "Вы получаете балл за создание, разметку текста и объяснение того, почему этот текст подходит для оценки (что в нём сложного). Всего за этот пункт 2 балла, т. к. языка 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KC9T6V43j1XF"
   },
   "source": [
    "**не буду при предобработке приводить тексты к нижнему регистру, так как это повлияет на распознавание названий и имен**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1u8nJXWhU7vp"
   },
   "source": [
    "#### русский текст"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfTT3e77U7vq"
   },
   "source": [
    "**объяснение почему тут трудные моменты для автоматического постэггинга:**\n",
    "\n",
    "\n",
    "слова с дефисами: корейско-российском, Президент-отеле, из-за, ОМОН-бойцов, загадочно-угрожающе, кто-то, 12-часовом\n",
    "\n",
    "придуманные слова: хакерята, хакерки, склипкими, мульками\n",
    "\n",
    "редкие слова: тюрбо, аррабьята, шаркающих, сапожонками, из-под тишка, внимать, сего\n",
    "\n",
    "имена/фамилии: М.А. Чёрнов, А. Н. Пагодный\n",
    "\n",
    "аббревиатуры: МФТИ, РГГУ;\n",
    "\n",
    "англ названия Gulfstream-G450\n",
    "\n",
    "общепринятые сокращения: и др.\n",
    "\n",
    "ученые - мб будет путаница adj/noun\n",
    "приготовленным, заинтересованным, Собравшиеся, выглядывающий - мб будет путаница adj/причастие\n",
    "\n",
    "наречия В потемках, изподтишка (неправильное написание)\n",
    "\n",
    "сами собой - местоимения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TrfXVjnSU7vr"
   },
   "outputs": [],
   "source": [
    "rus_text = '''Ученые выступят на 12-часовом Втором корейско-российском дне науки и технологий. Ой!\n",
    "Он состоится 30 мая в Президент-отеле. \n",
    "С докладами в сессии «Искусственный Интеллект» выступят заведующий лабораторией глубокого обучения МФТИ М.А. Чёрнов и профессор кафедры информационных систем РГГУ А. Н. Пагодный.\n",
    "Собравшиеся там хакерята, хакерки и др. представят свои наработки со всякими разными мульками. \n",
    "Прибыл кто-то крайне загадочно-угрожающе выглядывающий из-за штор своего личного Gulfstream-G.\n",
    "Обсуждая работу турбийона, гости под присмотром шаркающих склипкими сапожонками ОМОН-бойцов уселись на свои места. \n",
    "Путем принуждения изподтишка они продолжили внимать докладам.\n",
    "В завершении сего форума участники насладились прекрасно приготовленным тюрбо со спагетти аррабьята. \n",
    "В потемках все сами собой разошлись. Им же некогда. Было прочитано лучше. Как хороша!'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zUZkTdwcWex"
   },
   "source": [
    "размеченный текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nLr7Ek_U7vv"
   },
   "outputs": [],
   "source": [
    "rus_text_tagged = ['Ученые<NOUN>',\n",
    "'выступят<VERB>',\n",
    "'на<PREP>',\n",
    "'12-часовом<ADJ>',\n",
    "'Втором<ADJ>',\n",
    "'корейско-российском<ADJ>',\n",
    "'дне<NOUN>',\n",
    "'науки<NOUN>',\n",
    "'и<CONJ>',\n",
    "'технологий<NOUN>',\n",
    "'Ой<INTJ>',\n",
    "'Он<PRON>',\n",
    "'состоится<VERB>',\n",
    "'30<NUMR>',\n",
    "'мая<NOUN>',\n",
    "'в<PREP>',\n",
    "'Президент-отеле<NOUN>',\n",
    "'С<PREP>',\n",
    "'докладами<NOUN>',\n",
    "'в<PREP>',\n",
    "'сессии<NOUN>',\n",
    "'Искусственный<ADJ>',\n",
    "'Интеллект<NOUN>',\n",
    "'выступят<VERB>',\n",
    "'заведующий<NOUN>',\n",
    "'лабораторией<NOUN>',\n",
    "'глубокого<ADJ>',\n",
    "'обучения<NOUN>',\n",
    "'МФТИ<NOUN>',\n",
    "'М<NOUN>',\n",
    "'А<NOUN>',\n",
    "'Чёрнов<NOUN>',\n",
    "'и<CONJ>',\n",
    "'профессор<NOUN>',\n",
    "'кафедры<NOUN>',\n",
    "'информационных<ADJ>',\n",
    "'систем<NOUN>',\n",
    "'РГГУ<NOUN>',\n",
    "'А<NOUN>',\n",
    "'Н<NOUN>',\n",
    "'Пагодный<NOUN>',\n",
    "'Собравшиеся<ADJ>',\n",
    "'там<ADV>',\n",
    "'хакерята<NOUN>',\n",
    "'хакерки<NOUN>',\n",
    "'и<CONJ>',\n",
    "'др<PRON>',\n",
    "'представят<VERB>',\n",
    "'свои<PRON>',\n",
    "'наработки<NOUN>',\n",
    "'со<PREP>',\n",
    "'всякими<PRON>',\n",
    "'разными<ADJ>',\n",
    "'мульками<NOUN>',\n",
    "'Прибыл<VERB>',\n",
    "'кто-то<PRON>',\n",
    "'крайне<ADV>',\n",
    "'загадочно-угрожающе<ADV>',\n",
    "'выглядывающий<ADJ>',\n",
    "'из-за<PREP>',\n",
    "'штор<NOUN>',\n",
    "'своего<PRON>',\n",
    "'личного<ADJ>',\n",
    "'Gulfstream-G<NOUN>',\n",
    "'Обсуждая<VERB>',\n",
    "'работу<NOUN>',\n",
    "'турбийона<NOUN>',\n",
    "'гости<NOUN>',\n",
    "'под<PREP>',\n",
    "'присмотром<NOUN>',\n",
    "'шаркающих<ADJ>',\n",
    "'склипкими<ADJ>',\n",
    "'сапожонками<NOUN>',\n",
    "'ОМОН-бойцов<NOUN>',\n",
    "'уселись<VERB>',\n",
    "'на<PREP>',\n",
    "'свои<PRON>',\n",
    "'места<NOUN>',\n",
    "'Путем<ADV>',\n",
    "'принуждения<NOUN>',\n",
    "'изподтишка<ADV>',\n",
    "'они<PRON>',\n",
    "'продолжили<VERB>',\n",
    "'внимать<VERB>',\n",
    "'докладам<NOUN>',\n",
    "'В<PREP>',\n",
    "'завершении<NOUN>',\n",
    "'сего<PRON>',\n",
    "'форума<NOUN>',\n",
    "'участники<NOUN>',\n",
    "'насладились<VERB>',\n",
    "'прекрасно<ADV>',\n",
    "'приготовленным<ADJ>',\n",
    "'тюрбо<NOUN>',\n",
    "'со<PREP>',\n",
    "'спагетти<NOUN>',\n",
    "'аррабьята<NOUN>',\n",
    "'В<PREP>',\n",
    "'потемках<NOUN>',\n",
    "'все<PRON>',\n",
    "'сами<PRON>',\n",
    "'собой<PRON>',\n",
    "'разошлись<VERB>',\n",
    "'Им<PRON>',\n",
    "'же<PART>',\n",
    "'некогда<ADV>',\n",
    "'Было<VERB>',\n",
    "'прочитано<ADJ>',\n",
    "'лучше<ADV>',\n",
    "'Как<PART>',\n",
    "'хороша<ADJ>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuozI_UOU7vy"
   },
   "source": [
    "#### англ текст"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLGZUa1fU7vz"
   },
   "source": [
    "**объяснение почему тут трудные моменты для автоматического постэггинга:**\n",
    "\n",
    "фразовые глаголы: makes up, hold up\n",
    "\n",
    "дефисы: dwimmer-crafty, dumbbell-shaped, sometimes-complex-shaped, high-probability\n",
    "\n",
    "Восклицания: Alas, Oh, Ah\n",
    "\n",
    "сокращения: doesn’t\n",
    "\n",
    "редкие слова: dwimmer-crafty, foe, subshells\n",
    "\n",
    "названия: Bohr, S9, A, B, Z\n",
    "\n",
    "сокращения: etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "415GPbQPU7vz"
   },
   "outputs": [],
   "source": [
    "eng_text = '''Alas!\n",
    "Oh, ah!\n",
    "It is ill dealing with such a foe: he is a wizard both cunning and dwimmer-crafty, having many disguises.\n",
    "For instance, S9 subshells have a single, spherical orbital, while A, B, Z, etc. subshells contain three dumbbell-shaped orbitals at right angles to each other.\n",
    "The Bohr model is useful to explain the reactivity and chemical bonding of many elements, but it actually doesn’t give a very accurate description of how electrons are distributed in space around the nucleus.\n",
    "Specifically, electrons spend most of their time in sometimes-complex-shaped regions of space around the nucleus, known as electron orbitals.\n",
    "This high-probability region makes up an orbital, and each orbital can hold up to two electrons.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VsY47KjPU7v2"
   },
   "outputs": [],
   "source": [
    "eng_text_tagged = [\n",
    "'Alas<INTJ>',\n",
    "#'!',\n",
    "'Oh<INTJ>',\n",
    "#',',\n",
    "'ah<INTJ>',\n",
    "#'!',\n",
    "'It<PRON>',\n",
    "'is<VERB>',\n",
    "'ill<ADV>',\n",
    "'dealing<VERB>',\n",
    "'with<PREP>',\n",
    "'such<PRON>',\n",
    "'a<ART>',\n",
    "'foe<NOUN>',\n",
    "# ':',\n",
    "'he<PRON>',\n",
    "'is<VERB>',\n",
    "'a<ART>',\n",
    "'wizard<NOUN>',\n",
    "'both<PRON>',\n",
    "'cunning<ADJ>',\n",
    "'and<CONJ>',\n",
    "'dwimmer-crafty<ADJ>',\n",
    "# ',',\n",
    "'having<ADJ>',\n",
    "'many<PRON>',\n",
    "'disguises<NOUN>',\n",
    "# '.',\n",
    "'For<PREP>',\n",
    "'instance<NOUN>',\n",
    "# ',',\n",
    "'S9<NOUN>',\n",
    "'subshells<NOUN>',\n",
    "'have<VERB>',\n",
    "'a<ART>',\n",
    "'single<ADJ>',\n",
    "# ',',\n",
    "'spherical<ADJ>',\n",
    "'orbital<NOUN>',\n",
    "# ',',\n",
    "'while<CONJ>',\n",
    "'A<NOUN>',\n",
    "# ',',\n",
    "'B<NOUN>',\n",
    "# ',',\n",
    "'Z<NOUN>',\n",
    "# ',',\n",
    "'etc<ART>',\n",
    "# '.',\n",
    "'subshells<NOUN>',\n",
    "'contain<VERB>',\n",
    "'three<NUMB>',\n",
    "'dumbbell-shaped<ADJ>',\n",
    "'orbitals<NOUN>',\n",
    "'at<PREP>',\n",
    "'right<ADJ>',\n",
    "'angles<NOUN>',\n",
    "'to<PART>',\n",
    "'each<PRON>',\n",
    "'other<PRON>',\n",
    "# '.',\n",
    "'The<ART>',\n",
    "'Bohr<NOUN>',\n",
    "'model<NOUN>',\n",
    "'is<VERB>',\n",
    "'useful<ADJ>',\n",
    "'to<PART>',\n",
    "'explain<VERB>',\n",
    "'the<ART>',\n",
    "'reactivity<NOUN>',\n",
    "'and<CONJ>',\n",
    "'chemical<ADJ>',\n",
    "'bonding<NOUN>',\n",
    "'of<PREP>',\n",
    "'many<PRON>',\n",
    "'elements<NOUN>',\n",
    "# ',',\n",
    "'but<CONJ>',\n",
    "'it<PRON>',\n",
    "'actually<ADV>',\n",
    "'doesn<VERB>',\n",
    "# '’',\n",
    "'t<VERB>',\n",
    "'give<VERB>',\n",
    "'a<ART>',\n",
    "'very<ADV>',\n",
    "'accurate<ADJ>',\n",
    "'description<NOUN>',\n",
    "'of<PREP>',\n",
    "'how<ADV>',\n",
    "'electrons<NOUN>',\n",
    "'are<VERB>',\n",
    "'distributed<VERB>',\n",
    "'in<PREP>',\n",
    "'space<NOUN>',\n",
    "'around<PREP>',\n",
    "'the<ART>',\n",
    "'nucleus<NOUN>',\n",
    "# '.',\n",
    "'Specifically<ADV>',\n",
    "# ',',\n",
    "'they<PRON>',\n",
    "'spend<VERB>',\n",
    "'most<PRON>',\n",
    "'of<PREP>',\n",
    "'their<PRON>',\n",
    "'time<NOUN>',\n",
    "'in<PREP>',\n",
    "'sometimes-complex-shaped<ADJ>',\n",
    "'regions<NOUN>',\n",
    "'of<PREP>',\n",
    "'space<NOUN>',\n",
    "'around<PREP>',\n",
    "'the<ART>',\n",
    "'nucleus<NOUN>',\n",
    "# ',',\n",
    "'known<ADJ>',\n",
    "'as<CONJ>',\n",
    "'electron<ADJ>',\n",
    "'orbitals<NOUN>',\n",
    "# '.',\n",
    "'This<PRON>',\n",
    "'high-probability<ADJ>',\n",
    "'region<NOUN>',\n",
    "'makes<VERB>',\n",
    "'up<PART>',\n",
    "'an<ART>',\n",
    "'orbital<NOUN>',\n",
    "# ',',\n",
    "'and<CONJ>',\n",
    "'each<PRON>',\n",
    "'orbital<NOUN>',\n",
    "'can<VERB>',\n",
    "'hold<VERB>',\n",
    "'up<PART>',\n",
    "'to<PART>',\n",
    "'two<NUMB>',\n",
    "'electrons<NOUN>',\n",
    "# '.'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOJs9r2TU7v6"
   },
   "source": [
    "### сведение к единому набору тэгов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlEaew-SU7v6"
   },
   "source": [
    "в разных системах имена теггов и части речи  могут отличаться, – вам надо будет свести это всё к единому стандарту с помощью какой-то функции или кода и сравнить с вашим размеченным руками эталоном - тоже с помощью какого-то кода или функции. Этот пункт стоит 2 балла."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kifIXghAU7v8"
   },
   "source": [
    " *не буду учитывать разметку пунктуации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TllsOgrzi3fP"
   },
   "source": [
    "пунктуация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TXVERx0i3fQ"
   },
   "outputs": [],
   "source": [
    "punct = '.»,?!’«:\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8m1Cgam-U7xI"
   },
   "source": [
    "соотв тегов (для русского, для англа будет ниже)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ku7Mg4FLU7xI"
   },
   "outputs": [],
   "source": [
    "pos_dict = {\n",
    "    \"LATN\": \"NOUN\", \n",
    "    \"PROPN\": \"NOUN\", \n",
    "    \"S\": \"NOUN\",\n",
    "\n",
    "    \"INFN\": \"VERB\",\n",
    "    \"GRND\": \"VERB\",\n",
    "    \"AUX\": \"VERB\",\n",
    "    \"V\": \"VERB\",\n",
    "\n",
    "    \"ADJF\": \"ADJ\",\n",
    "    \"ADJS\": \"ADJ\",\n",
    "    \"PRTF\": \"ADJ\",\n",
    "    \"PRTS\": \"ADJ\",\n",
    "    \"A\": \"ADJ\",\n",
    "\n",
    "    \"ADVB\": \"ADV\",\n",
    "    \"COMP\": \"ADV\",\n",
    "    \"PRED\": \"ADV\",\n",
    "\n",
    "    \"NPRO\": \"PRON\",\n",
    "    \"DET\": \"PRON\",\n",
    "    \"ADVPRO\": \"PRON\",\n",
    "    \"SPRO\": \"PRON\",\n",
    "    \"APRO\": \"PRON\",\n",
    "\n",
    "    \"NUMB\": \"NUMR\",\n",
    "    \"NUMB,intg\": \"NUMR\",\n",
    "    \"NUMB,real\": \"NUMR\",\n",
    "    \"ROMN\": \"NUMR\",\n",
    "    \"NUM\": \"NUMR\",\n",
    "    \"ANUM\": \"NUMR\",\n",
    "\n",
    "    \"PRCL\": \"PART\",\n",
    "\n",
    "    \"CCONJ\": \"CONJ\",\n",
    "    \"SCONJ\": \"CONJ\",\n",
    "\n",
    "    \"ADP\": \"PREP\",\n",
    "    \"PR\": \"PREP\",  \n",
    "    \n",
    "    \n",
    "    \"CC\": \"CONJ\",\n",
    " \"CD\": \"NUMB\",\n",
    " \"DT\": \"ART\",\n",
    " \"IN\": \"PREP\",\n",
    " \"JJ\": \"ADJ\",\n",
    " \"JJS\": \"ADJ\",\n",
    " \"MD\": \"VERB\",\n",
    " \"NN\": \"NOUN\",\n",
    " \"NNP\": \"NOUN\",\n",
    " \"NNS\": \"NOUN\",\n",
    " \"PDT\": \"PRON\",\n",
    " \"PRP\": \"PRON\",\n",
    " \"PRP$\": \"PRON\",\n",
    " \"RB\": \"ADV\",\n",
    " \"RP\": \"PART\",\n",
    " \"TO\": \"PART\",\n",
    " \"UH\": \"INTJ\",\n",
    " \"VB\": \"VERB\",\n",
    " \"VBG\": \"VERB\",\n",
    " \"VBN\": \"VERB\",\n",
    " \"VBP\": \"VERB\",\n",
    " \"VBZ\": \"VERB\",\n",
    " \"WRB\": \"ADV\",\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXrlUMqqU7xE"
   },
   "source": [
    " ф-я сопоставляет теги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T24NiOAfU7xE"
   },
   "outputs": [],
   "source": [
    "def get_tag(pos, dictt=pos_dict):\n",
    "    if pos in dictt:\n",
    "        return dictt[pos]\n",
    "    else:\n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfUHjbKTU7xZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhcIyDUMU7v8"
   },
   "source": [
    "## прогон трех  POS теггеров для русского"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TgnGeTOeMom"
   },
   "source": [
    "### accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcSR8mQ_d5CY"
   },
   "outputs": [],
   "source": [
    "def accuracy_check(results, gold):\n",
    "    print(\"Accuracy: %.4f\" % accuracy_score(results, gold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0o-BIph-U7v9"
   },
   "source": [
    "### 1. pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tKrUjhSCVTzv",
    "outputId": "f87fb2a9-6dff-4be4-9fa2-6f694c45c4db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/57/b2ff2fae3376d4f3c697b9886b64a54b476e1a332c67eee9f88e7f1ae8c9/pymorphy2-0.9.1-py3-none-any.whl (55kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 1.8MB/s \n",
      "\u001b[?25hCollecting pymorphy2-dicts-ru<3.0,>=2.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/9b/358faaff410f65a4ad159275e897b5956dcb20576c5b8e764b971c1634d7/pymorphy2_dicts_ru-2.4.404381.4453942-py2.py3-none-any.whl (8.0MB)\n",
      "\u001b[K     |████████████████████████████████| 8.0MB 6.4MB/s \n",
      "\u001b[?25hCollecting dawg-python>=0.7.1\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.6.2)\n",
      "Installing collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.404381.4453942\n"
     ]
    }
   ],
   "source": [
    " ! pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45Oi6xAYU7v_"
   },
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-uYC0ipU7wC"
   },
   "outputs": [],
   "source": [
    "from pymorphy2.tokenizers import simple_word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vY7XE3x5U7wF"
   },
   "source": [
    "токенизированный текст\n",
    "\n",
    "(токены разбиваются по пробелам и тд. по дефисам НЕ разбиваются)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJI1MsbnU7wJ"
   },
   "outputs": [],
   "source": [
    "tokenized_rus = simple_word_tokenize(rus_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5qqoUffugDLI"
   },
   "outputs": [],
   "source": [
    "pymorphy_res = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQhpaF-2U7wM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for token in tokenized_rus:\n",
    "    if token in punct:\n",
    "        continue\n",
    "    # беру первый, самый вероятный разбор\n",
    "    pars_res = morph.parse(token)[0]\n",
    "    pos = pars_res.tag.POS\n",
    "    if not pos: # if pos == none\n",
    "        pos = pars_res.tag\n",
    "\n",
    "    ## sootv of tags\n",
    "    res_pos = str(get_tag(pos))\n",
    "    res_token = token + '<' + res_pos + '>'\n",
    "    pymorphy_res.append(res_token)\n",
    "    \n",
    "#     print(pos, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPOxmIeHU7wQ",
    "outputId": "5459ff60-c72f-45c5-eefb-7281f73906d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ученые<NOUN>',\n",
       " 'выступят<VERB>',\n",
       " 'на<PREP>',\n",
       " '12-часовом<ADJ>',\n",
       " 'Втором<NOUN>',\n",
       " 'корейско-российском<ADJ>',\n",
       " 'дне<NOUN>',\n",
       " 'науки<NOUN>',\n",
       " 'и<CONJ>',\n",
       " 'технологий<NOUN>',\n",
       " 'Ой<INTJ>',\n",
       " 'Он<PRON>',\n",
       " 'состоится<VERB>',\n",
       " '30<NUMB,intg>',\n",
       " 'мая<NOUN>',\n",
       " 'в<PREP>',\n",
       " 'Президент-отеле<NOUN>',\n",
       " 'С<PREP>',\n",
       " 'докладами<NOUN>',\n",
       " 'в<PREP>',\n",
       " 'сессии<NOUN>',\n",
       " 'Искусственный<ADJ>',\n",
       " 'Интеллект<NOUN>',\n",
       " 'выступят<VERB>',\n",
       " 'заведующий<ADJ>',\n",
       " 'лабораторией<NOUN>',\n",
       " 'глубокого<ADJ>',\n",
       " 'обучения<NOUN>',\n",
       " 'МФТИ<NOUN>',\n",
       " 'М<NOUN>',\n",
       " 'А<CONJ>',\n",
       " 'Чёрнов<ADJ>',\n",
       " 'и<CONJ>',\n",
       " 'профессор<NOUN>',\n",
       " 'кафедры<NOUN>',\n",
       " 'информационных<ADJ>',\n",
       " 'систем<NOUN>',\n",
       " 'РГГУ<NOUN>',\n",
       " 'А<CONJ>',\n",
       " 'Н<ADJ>',\n",
       " 'Пагодный<ADJ>',\n",
       " 'Собравшиеся<ADJ>',\n",
       " 'там<ADV>',\n",
       " 'хакерята<NOUN>',\n",
       " 'хакерки<NOUN>',\n",
       " 'и<CONJ>',\n",
       " 'др<NOUN>',\n",
       " 'представят<VERB>',\n",
       " 'свои<ADJ>',\n",
       " 'наработки<NOUN>',\n",
       " 'со<PREP>',\n",
       " 'всякими<ADJ>',\n",
       " 'разными<ADJ>',\n",
       " 'мульками<NOUN>',\n",
       " 'Прибыл<VERB>',\n",
       " 'кто-то<PRON>',\n",
       " 'крайне<ADV>',\n",
       " 'загадочно-угрожающе<ADV>',\n",
       " 'выглядывающий<ADJ>',\n",
       " 'из-за<PREP>',\n",
       " 'штор<NOUN>',\n",
       " 'своего<ADJ>',\n",
       " 'личного<ADJ>',\n",
       " 'Gulfstream-G450<LATN>',\n",
       " 'Обсуждая<VERB>',\n",
       " 'работу<NOUN>',\n",
       " 'турбийона<NOUN>',\n",
       " 'гости<NOUN>',\n",
       " 'под<PREP>',\n",
       " 'присмотром<NOUN>',\n",
       " 'шаркающих<ADJ>',\n",
       " 'склипкими<ADJ>',\n",
       " 'сапожонками<NOUN>',\n",
       " 'ОМОН-бойцов<NOUN>',\n",
       " 'уселись<VERB>',\n",
       " 'на<PREP>',\n",
       " 'свои<ADJ>',\n",
       " 'места<NOUN>',\n",
       " 'Путем<PREP>',\n",
       " 'принуждения<NOUN>',\n",
       " 'изподтишка<NOUN>',\n",
       " 'они<PRON>',\n",
       " 'продолжили<VERB>',\n",
       " 'внимать<VERB>',\n",
       " 'докладам<NOUN>',\n",
       " 'В<PREP>',\n",
       " 'завершении<NOUN>',\n",
       " 'сего<ADJ>',\n",
       " 'форума<NOUN>',\n",
       " 'участники<NOUN>',\n",
       " 'насладились<VERB>',\n",
       " 'прекрасно<ADV>',\n",
       " 'приготовленным<ADJ>',\n",
       " 'тюрбо<ADV>',\n",
       " 'со<PREP>',\n",
       " 'спагетти<NOUN>',\n",
       " 'аррабьята<NOUN>',\n",
       " 'В<PREP>',\n",
       " 'потемках<NOUN>',\n",
       " 'все<ADJ>',\n",
       " 'сами<ADJ>',\n",
       " 'собой<PRON>',\n",
       " 'разошлись<VERB>',\n",
       " 'Им<NOUN>',\n",
       " 'же<PART>',\n",
       " 'некогда<ADV>',\n",
       " 'Было<VERB>',\n",
       " 'прочитано<ADJ>',\n",
       " 'лучше<ADV>',\n",
       " 'Как<CONJ>',\n",
       " 'хороша<ADJ>']"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pymorphy_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqCiGNEeU7wX"
   },
   "source": [
    "оценка accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-uuV084NU7wY",
    "outputId": "c97540a1-2fc4-46b6-cf6d-1f7bc1410f5a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8018\n"
     ]
    }
   ],
   "source": [
    "accuracy_check(pymorphy_res, rus_text_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewA9lvz_i3gq",
    "outputId": "fe37010b-dce6-4e98-9b25-bc8d06510728"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pymorphy_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eKUJbZ3mi3gt",
    "outputId": "28e5d24e-5660-495d-c58f-1eb3be958981"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rus_text_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzcdifk6U7wb"
   },
   "source": [
    "### 2. Natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "collapsed": true,
    "id": "0qR_VmrIU7wb",
    "outputId": "d3d5b943-6361-45cd-a892-1e0af366ae55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting natasha\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/34/9abb6b5c95993001518e517f21157e2c955749ac4f3c79dc3c2cf25e72fe/natasha-1.3.0-py3-none-any.whl (34.4MB)\n",
      "\u001b[K     |████████████████████████████████| 34.4MB 111kB/s \n",
      "\u001b[?25hCollecting ipymarkup>=0.8.0\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/9b/bf54c98d50735a4a7c84c71e92c5361730c878ebfe903d2c2d196ef66055/ipymarkup-0.9.0-py3-none-any.whl\n",
      "Collecting razdel>=0.5.0\n",
      "  Downloading https://files.pythonhosted.org/packages/15/2c/664223a3924aa6e70479f7d37220b3a658765b9cfe760b4af7ffdc50d38f/razdel-0.5.0-py3-none-any.whl\n",
      "Collecting yargy>=0.14.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/07/94306844e3a5cb520660612ad98bce56c168edb596679bd541e68dfde089/yargy-0.14.0-py3-none-any.whl (41kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 4.9MB/s \n",
      "\u001b[?25hCollecting slovnet>=0.3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/6f/1c989335c9969421f771e4f0410ba70d82fe992ec9f3cbac9f432d8f5733/slovnet-0.4.0-py3-none-any.whl (49kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 5.3MB/s \n",
      "\u001b[?25hCollecting pymorphy2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/57/b2ff2fae3376d4f3c697b9886b64a54b476e1a332c67eee9f88e7f1ae8c9/pymorphy2-0.9.1-py3-none-any.whl (55kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 5.5MB/s \n",
      "\u001b[?25hCollecting navec>=0.9.0\n",
      "  Downloading https://files.pythonhosted.org/packages/83/ad/554945ebee66fe83fefd61e043938981dd9e6136882025c506ac6faa6a4c/navec-0.9.0-py3-none-any.whl\n",
      "Collecting intervaltree>=3\n",
      "  Downloading https://files.pythonhosted.org/packages/50/fb/396d568039d21344639db96d940d40eb62befe704ef849b27949ded5c3bb/intervaltree-3.1.0.tar.gz\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from slovnet>=0.3.0->natasha) (1.18.5)\n",
      "Collecting dawg-python>=0.7.1\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2->natasha) (0.6.2)\n",
      "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/79/bea0021eeb7eeefde22ef9e96badf174068a2dd20264b9a378f2be1cdd9e/pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2MB)\n",
      "\u001b[K     |████████████████████████████████| 8.2MB 28.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.2.2)\n",
      "Building wheels for collected packages: intervaltree\n",
      "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26100 sha256=5831eba465e7157682187392a6f643eac7df08e28e6aa25c89b0689d6693c3db\n",
      "  Stored in directory: /root/.cache/pip/wheels/f3/f2/66/e9c30d3e9499e65ea2fa0d07c002e64de63bd0adaa49c445bf\n",
      "Successfully built intervaltree\n",
      "Installing collected packages: intervaltree, ipymarkup, razdel, dawg-python, pymorphy2-dicts-ru, pymorphy2, yargy, navec, slovnet, natasha\n",
      "  Found existing installation: intervaltree 2.1.0\n",
      "    Uninstalling intervaltree-2.1.0:\n",
      "      Successfully uninstalled intervaltree-2.1.0\n",
      "Successfully installed dawg-python-0.7.2 intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.3.0 navec-0.9.0 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 razdel-0.5.0 slovnet-0.4.0 yargy-0.14.0\n"
     ]
    }
   ],
   "source": [
    "! pip install natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "faVnhY54U7we"
   },
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-Jzdb4uU7wi"
   },
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "\n",
    "doc = Doc(rus_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fsyCSoUMU7wo"
   },
   "outputs": [],
   "source": [
    "doc.segment(segmenter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7R69nLkii3hN"
   },
   "outputs": [],
   "source": [
    "doc.tag_morph(morph_tagger)\n",
    "\n",
    "natasha_res = [_.text + '<' + get_tag(_.pos) + '>' for _ in doc.tokens if _.text not in punct]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlwDUuwPU7ws"
   },
   "source": [
    "оценка accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8P11mOPU7ws",
    "outputId": "73e02ee3-22d7-46f6-ede1-26d170dbe42d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8739\n"
     ]
    }
   ],
   "source": [
    "accuracy_check(natasha_res, rus_text_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YE8RkbFMU7wv"
   },
   "source": [
    "### 3. mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_1hPgkHU7ww"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxPZcVRtU7wz"
   },
   "outputs": [],
   "source": [
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTSoeDLpU7w9"
   },
   "outputs": [],
   "source": [
    "mystem_res = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSJdTqDnU7xB"
   },
   "outputs": [],
   "source": [
    "for token in tokenized_rus:\n",
    "    if token in punct:\n",
    "        continue\n",
    "    r = m.analyze(token)\n",
    "    pos = ''\n",
    "    for el in r:\n",
    "        if 'analysis' in el:\n",
    "            if el['analysis']:\n",
    "                gram = el['analysis'][0]['gr']\n",
    "                pos0 = re.search('[^,=]+',gram).group()\n",
    "                if pos == '':\n",
    "                    pos = pos0\n",
    "                elif pos == pos0:\n",
    "                    pos = pos0\n",
    "                else: # аннотация частей не совпала\n",
    "                    pos = ''\n",
    "                    break\n",
    "            else: # анализ пуст\n",
    "                continue\n",
    "        else: # нет анализа\n",
    "            continue\n",
    "    res_pos = get_tag(pos)\n",
    "    res_token = token + '<' + res_pos + '>'\n",
    "    mystem_res.append(res_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "BEaiPDlLovtm",
    "outputId": "75a6c2e9-82a6-438d-b6c2-c6a0a917d21f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mystem_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PX_q40kYU7xY"
   },
   "source": [
    "оценка accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQBSsdS6fJyo",
    "outputId": "36024713-8f2a-4d41-ff13-c72da9556f0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8108\n"
     ]
    }
   ],
   "source": [
    "accuracy_check(mystem_res, rus_text_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhPD2ODBU7xd"
   },
   "source": [
    "## прогон трех  POS теггеров для английского"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2lIMo8ttDo0"
   },
   "source": [
    "сопоставление тегов для англа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zShgeSzXi3ho"
   },
   "outputs": [],
   "source": [
    "td_eng = {\n",
    "    \"CC\": \"CONJ\",\n",
    " \"CD\": \"NUMB\",\n",
    " \"DT\": \"ART\",\n",
    " \"IN\": \"PREP\",\n",
    " \"JJ\": \"ADJ\",\n",
    " \"JJS\": \"ADJ\",\n",
    " \"MD\": \"VERB\",\n",
    " \"NN\": \"NOUN\",\n",
    " \"NNP\": \"NOUN\",\n",
    " \"NNS\": \"NOUN\",\n",
    " \"PDT\": \"PRON\",\n",
    " \"PRP\": \"PRON\",\n",
    " \"PRP$\": \"PRON\",\n",
    " \"RB\": \"ADV\",\n",
    " \"RP\": \"PART\",\n",
    " \"TO\": \"PART\",\n",
    " \"UH\": \"INTJ\",\n",
    " \"VB\": \"VERB\",\n",
    " \"VBG\": \"VERB\",\n",
    " \"VBN\": \"VERB\",\n",
    " \"VBP\": \"VERB\",\n",
    " \"VBZ\": \"VERB\",\n",
    " \"WRB\": \"ADV\",\n",
    " \n",
    " \n",
    " \"ADP\": \"PREP\",\n",
    " \"AUX\": \"VERB\",\n",
    " \"CCONJ\": \"CONJ\",\n",
    " \"SCONJ\": \"CONJ\",\n",
    " \"DET\": \"ART\",\n",
    " \"NUM\": \"NUMB\",\n",
    " \"PROPN\": \"NOUN\",\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3l_0hGHPb225"
   },
   "source": [
    "### 1. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5oG_QvnU7xe"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bvth0pARU7xh"
   },
   "outputs": [],
   "source": [
    "tokenized_eng = word_tokenize(eng_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "создаю список где каждый элемент это слово а рядом в <> его часть речи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SwmJ-D-i3h5"
   },
   "outputs": [],
   "source": [
    "res0 = nltk.pos_tag(tokenized_eng)\n",
    "nltk_res = [pair[0] + '<' + get_tag(pair[1], td_eng) + '>' for pair in res0 if pair[0] not in punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JTnW9eeyi3h-",
    "outputId": "0ecff7c4-01db-4608-aed9-18c4fa3181ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117, 117)"
      ]
     },
     "execution_count": 104,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk_res), len(eng_text_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DB-GDkbvU7xq"
   },
   "source": [
    "оценка accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAptJlIIU7xq",
    "outputId": "923971a5-9f03-48e7-fd5a-a0b7e1e4bd7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8205\n"
     ]
    }
   ],
   "source": [
    "accuracy_check(nltk_res, eng_text_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eYDvHgGU7xx"
   },
   "source": [
    "### SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "kQHKFUjCU7xy",
    "outputId": "72fa25f7-4edd-4881-de18-5d569235dc5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: thinc==7.4.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.43.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.16.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (41.0.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.6.16)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\asus\\anaconda3\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (0.5.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKi1NpPWU7x2"
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GUWyzjc-U7x7"
   },
   "outputs": [],
   "source": [
    "# Загружаем весь пайплайн для английского\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXp9KyXRi3iQ"
   },
   "outputs": [],
   "source": [
    "spacy_res = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "euo4KaW6U7yB"
   },
   "outputs": [],
   "source": [
    "# Обрабатываем текст\n",
    "for token in tokenized_eng:\n",
    "    if token in punct:\n",
    "        continue\n",
    "    doc = nlp(token)\n",
    "    pos = ''\n",
    "    for t in doc:\n",
    "        pos0 = t.pos_\n",
    "        if pos == '':\n",
    "            pos = pos0\n",
    "        elif pos == pos0:\n",
    "            pos = pos0\n",
    "        else: # аннотация частей не совпала\n",
    "            pos = ''\n",
    "            break\n",
    "            \n",
    "    spacy_res.append(token + '<' + get_tag(pos, td_eng) + '>')\n",
    "#     print(t.text, t.pos_, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aO1d7MUFi3iX",
    "outputId": "47c9c554-0080-485c-a9b0-c34b54db2b46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 112,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spacy_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1RgD6ew3U7yJ",
    "outputId": "ab7225f2-91d7-41c1-ef68-33644479a03b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alas<INTJ>',\n",
       " 'Oh<INTJ>',\n",
       " 'ah<INTJ>',\n",
       " 'It<PRON>',\n",
       " 'is<VERB>',\n",
       " 'ill<NOUN>',\n",
       " 'dealing<VERB>',\n",
       " 'with<PREP>',\n",
       " 'such<ADJ>',\n",
       " 'a<X>',\n",
       " 'foe<NOUN>',\n",
       " 'he<PRON>',\n",
       " 'is<VERB>',\n",
       " 'a<X>',\n",
       " 'wizard<NOUN>',\n",
       " 'both<ART>',\n",
       " 'cunning<ADJ>',\n",
       " 'and<CONJ>',\n",
       " 'dwimmer-crafty<>',\n",
       " 'having<VERB>',\n",
       " 'many<ADJ>',\n",
       " 'disguises<NOUN>',\n",
       " 'For<PREP>',\n",
       " 'instance<NOUN>',\n",
       " 'S9<NOUN>',\n",
       " 'subshells<NOUN>',\n",
       " 'have<VERB>',\n",
       " 'a<X>',\n",
       " 'single<ADJ>',\n",
       " 'spherical<ADJ>',\n",
       " 'orbital<NOUN>',\n",
       " 'while<CONJ>',\n",
       " 'A<ART>',\n",
       " 'B<NOUN>',\n",
       " 'Z<NOUN>',\n",
       " 'etc<X>',\n",
       " 'subshells<NOUN>',\n",
       " 'contain<VERB>',\n",
       " 'three<NUMB>',\n",
       " 'dumbbell-shaped<>',\n",
       " 'orbitals<NOUN>',\n",
       " 'at<PREP>',\n",
       " 'right<INTJ>',\n",
       " 'angles<NOUN>',\n",
       " 'to<PREP>',\n",
       " 'each<ART>',\n",
       " 'other<ADJ>',\n",
       " 'The<ART>',\n",
       " 'Bohr<NOUN>',\n",
       " 'model<NOUN>',\n",
       " 'is<VERB>',\n",
       " 'useful<ADJ>',\n",
       " 'to<PREP>',\n",
       " 'explain<VERB>',\n",
       " 'the<ART>',\n",
       " 'reactivity<NOUN>',\n",
       " 'and<CONJ>',\n",
       " 'chemical<NOUN>',\n",
       " 'bonding<VERB>',\n",
       " 'of<PREP>',\n",
       " 'many<ADJ>',\n",
       " 'elements<NOUN>',\n",
       " 'but<CONJ>',\n",
       " 'it<PRON>',\n",
       " 'actually<ADV>',\n",
       " 'doesn<NOUN>',\n",
       " 't<X>',\n",
       " 'give<VERB>',\n",
       " 'a<X>',\n",
       " 'very<ADV>',\n",
       " 'accurate<ADJ>',\n",
       " 'description<NOUN>',\n",
       " 'of<PREP>',\n",
       " 'how<ADV>',\n",
       " 'electrons<NOUN>',\n",
       " 'are<VERB>',\n",
       " 'distributed<VERB>',\n",
       " 'in<PREP>',\n",
       " 'space<NOUN>',\n",
       " 'around<PREP>',\n",
       " 'the<ART>',\n",
       " 'nucleus<NOUN>',\n",
       " 'Specifically<ADV>',\n",
       " 'electrons<NOUN>',\n",
       " 'spend<VERB>',\n",
       " 'most<ADJ>',\n",
       " 'of<PREP>',\n",
       " 'their<ART>',\n",
       " 'time<NOUN>',\n",
       " 'in<PREP>',\n",
       " 'sometimes-complex-shaped<>',\n",
       " 'regions<NOUN>',\n",
       " 'of<PREP>',\n",
       " 'space<NOUN>',\n",
       " 'around<PREP>',\n",
       " 'the<ART>',\n",
       " 'nucleus<NOUN>',\n",
       " 'known<VERB>',\n",
       " 'as<CONJ>',\n",
       " 'electron<NOUN>',\n",
       " 'orbitals<NOUN>',\n",
       " 'This<ART>',\n",
       " 'high-probability<>',\n",
       " 'region<NOUN>',\n",
       " 'makes<VERB>',\n",
       " 'up<PREP>',\n",
       " 'an<ART>',\n",
       " 'orbital<NOUN>',\n",
       " 'and<CONJ>',\n",
       " 'each<ART>',\n",
       " 'orbital<NOUN>',\n",
       " 'can<VERB>',\n",
       " 'hold<VERB>',\n",
       " 'up<PREP>',\n",
       " 'to<PREP>',\n",
       " 'two<NUMB>',\n",
       " 'electrons<NOUN>']"
      ]
     },
     "execution_count": 114,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S41qyjOUU7yN"
   },
   "source": [
    "оценка accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRMmhc9nU7yN",
    "outputId": "f7965cad-e6e1-442a-9eae-38e3509af0e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7009\n"
     ]
    }
   ],
   "source": [
    "accuracy_check(spacy_res, eng_text_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qS-_7SXDU7yQ"
   },
   "source": [
    "### Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bg6oOB5qU7yT",
    "outputId": "b85d21ab-c14f-4f6c-9923-91ecb1b15997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flair\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/19/902d1691c1963ab8c9a9578abc2d65c63aa1ecf4f8200143b5ef91ace6f5/flair-0.6.1-py3-none-any.whl (331kB)\n",
      "\u001b[K     |████████████████████████████████| 337kB 2.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair) (2019.12.20)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.7)\n",
      "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
      "Collecting ftfy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 5.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.2.2)\n",
      "Collecting segtok>=1.5.7\n",
      "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
      "Collecting langdetect\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
      "\u001b[K     |████████████████████████████████| 983kB 8.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from flair) (4.2.6)\n",
      "Collecting konoha<5.0.0,>=4.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/ea/01/47358efec5396fc80f98273c42cbdfe7aab056252b07884ffcc0f118978f/konoha-4.6.2-py3-none-any.whl\n",
      "Collecting pytest>=5.3.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/36/9e022b76a3ac440e1d750c64fa6152469f988efe0c568b945e396e2693b5/pytest-6.1.1-py3-none-any.whl (272kB)\n",
      "\u001b[K     |████████████████████████████████| 276kB 16.6MB/s \n",
      "\u001b[?25hCollecting janome\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n",
      "\u001b[K     |████████████████████████████████| 19.7MB 1.2MB/s \n",
      "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 47.0MB/s \n",
      "\u001b[?25hCollecting transformers>=3.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 40.4MB/s \n",
      "\u001b[?25hCollecting deprecated>=1.2.4\n",
      "  Downloading https://files.pythonhosted.org/packages/76/a1/05d7f62f956d77b23a640efc650f80ce24483aa2f85a09c03fb64f49e879/Deprecated-1.2.10-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.41.1)\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.4)\n",
      "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.22.2.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair) (2.8.1)\n",
      "Collecting sqlitedict>=1.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/2d/b1d99e9ad157dd7de9cd0d36a8a5876b13b55e4b75f7498bc96035fb4e96/sqlitedict-1.7.0.tar.gz\n",
      "Collecting mpld3==0.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
      "\u001b[K     |████████████████████████████████| 798kB 40.2MB/s \n",
      "\u001b[?25hCollecting bpemb>=0.3.2\n",
      "  Downloading https://files.pythonhosted.org/packages/91/77/3f0f53856e86af32b1d3c86652815277f7b5f880002584eb30db115b6df5/bpemb-0.3.2-py3-none-any.whl\n",
      "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.6.0+cu101)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.15.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (2.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.18.5)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->flair) (0.2.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.2.0)\n",
      "Collecting overrides==3.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/42/8d/caa729f809ecdf8e76fac3c1ff7d3f0b72c398c9dd8a6919927a30a873b3/overrides-3.0.0.tar.gz\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (2.0.0)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (0.10.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (20.2.0)\n",
      "Collecting pluggy<1.0,>=0.12\n",
      "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (20.4)\n",
      "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.9.0)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.0.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (3.0.12)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 23.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (0.7)\n",
      "Collecting tokenizers==0.8.1.rc2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 36.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (2.23.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
      "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (3.11.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.5)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair) (0.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.3.2->flair) (3.2.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.0.0->flair) (7.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.0->flair) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.0->flair) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.0->flair) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.0->flair) (3.0.4)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n",
      "Building wheels for collected packages: ftfy, segtok, langdetect, sqlitedict, mpld3, overrides, sacremoses\n",
      "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ftfy: filename=ftfy-5.8-cp36-none-any.whl size=45612 sha256=69e714bab130f6a2cfce435e413b1680907a24de3cdefab6c3e288d46ac16fe9\n",
      "  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n",
      "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for segtok: filename=segtok-1.5.10-cp36-none-any.whl size=25021 sha256=9e485d8c682e79337fdb4869cba15145b43f9a8decb4bdc37920a939353c0c56\n",
      "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993195 sha256=2aa00c93dd836be6c0934b1140887e5ac0d0678de4d396a914269454e829fa79\n",
      "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-cp36-none-any.whl size=14377 sha256=aa4efa52fec1fbfa3a43921dafa7550a68726b7074c156bbe88199d8dfd3b9b7\n",
      "  Stored in directory: /root/.cache/pip/wheels/cf/c6/4f/2c64a43f041415eb8b8740bd80e15e92f0d46c5e464d8e4b9b\n",
      "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for mpld3: filename=mpld3-0.3-cp36-none-any.whl size=116677 sha256=aba6edd871a4e9c612a2027aa36defee9d0d492ab6c6313ff43c86a74ae2b1e4\n",
      "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
      "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for overrides: filename=overrides-3.0.0-cp36-none-any.whl size=5669 sha256=b2718f33e14aef9a6f552f3974dd3045419cc16da11f389f443a9aa77c73a2f9\n",
      "  Stored in directory: /root/.cache/pip/wheels/6f/1b/ec/6c71a1eb823df7f850d956b2d8c50a6d49c191e1063d73b9be\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=501d6f502b6cc5ca60860b394f42c0591ad32f9f49e4568f98dc62e50ecf2988\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built ftfy segtok langdetect sqlitedict mpld3 overrides sacremoses\n",
      "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "Installing collected packages: ftfy, segtok, langdetect, overrides, konoha, pluggy, pytest, janome, sentencepiece, sacremoses, tokenizers, transformers, deprecated, sqlitedict, mpld3, bpemb, flair\n",
      "  Found existing installation: pluggy 0.7.1\n",
      "    Uninstalling pluggy-0.7.1:\n",
      "      Successfully uninstalled pluggy-0.7.1\n",
      "  Found existing installation: pytest 3.6.4\n",
      "    Uninstalling pytest-3.6.4:\n",
      "      Successfully uninstalled pytest-3.6.4\n",
      "Successfully installed bpemb-0.3.2 deprecated-1.2.10 flair-0.6.1 ftfy-5.8 janome-0.4.1 konoha-4.6.2 langdetect-1.0.8 mpld3-0.3 overrides-3.0.0 pluggy-0.13.1 pytest-6.1.1 sacremoses-0.0.43 segtok-1.5.10 sentencepiece-0.1.91 sqlitedict-1.7.0 tokenizers-0.8.1rc2 transformers-3.3.1\n"
     ]
    }
   ],
   "source": [
    "! pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tb6zd8T6U7yX"
   },
   "outputs": [],
   "source": [
    "import flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hy-Z142sU7yZ"
   },
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "57z8sqDMvavV",
    "outputId": "d5420d4c-0f94-45cf-a1d6-595e8338398a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-12 12:16:54,056 loading file /root/.flair/models/en-pos-ontonotes-fast-v0.5.pt\n"
     ]
    }
   ],
   "source": [
    "pos = SequenceTagger.load('pos-fast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "L3BROw2Jvbs4"
   },
   "outputs": [],
   "source": [
    "\n",
    "fl_tag_sents = []\n",
    "eng_text = re.sub(r'[^-’\\w\\s]','',eng_text) # убираю пунктуацию\n",
    "for sent in eng_text.split('\\n'):\n",
    "    sentence = Sentence(sent)\n",
    "    pos.predict(sentence)\n",
    "    ress = re.sub(' <', '<', sentence.to_tagged_string())\n",
    "    fl_tag_sents.append(ress)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "0ftq33xzwqbr",
    "outputId": "54b22f85-7083-4da2-daad-d3ba69862a21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alas<UH>',\n",
       " 'Oh<UH> ah<UH>',\n",
       " 'It<PRP> is<VBZ> ill<JJ> dealing<VBG> with<IN> such<PDT> a<DT> foe<NN> he<PRP> is<VBZ> a<DT> wizard<NN> both<CC> cunning<VBG> and<CC> dwimmer-crafty<JJ> having<VBG> many<JJ> disguises<NNS>',\n",
       " 'For<IN> instance<NN> S9<NNP> subshells<NNS> have<VBP> a<DT> single<JJ> spherical<JJ> orbital<NN> while<IN> A<NN> B<NNP> Z<NNP> etc<FW> subshells<NNS> contain<VBP> three<CD> dumbbell-shaped<VBN> orbitals<NNS> at<IN> right<JJ> angles<NNS> to<IN> each<DT> other<JJ>',\n",
       " 'The<DT> Bohr<NNP> model<NN> is<VBZ> useful<JJ> to<TO> explain<VB> the<DT> reactivity<NN> and<CC> chemical<NN> bonding<NN> of<IN> many<JJ> elements<NNS> but<CC> it<PRP> actually<RB> does<VBZ> n’t<RB> give<VB> a<DT> very<RB> accurate<JJ> description<NN> of<IN> how<WRB> electrons<NNS> are<VBP> distributed<VBN> in<IN> space<NN> around<IN> the<DT> nucleus<NN>',\n",
       " 'Specifically<RB> electrons<NNS> spend<VBP> most<JJS> of<IN> their<PRP$> time<NN> in<IN> sometimes-complex-shaped<JJ> regions<NNS> of<IN> space<NN> around<IN> the<DT> nucleus<NN> known<VBN> as<IN> electron<NN> orbitals<NNS>',\n",
       " 'This<DT> high-probability<JJ> region<NN> makes<VBZ> up<RP> an<DT> orbital<JJ> and<CC> each<DT> orbital<NN> can<MD> hold<VB> up<RP> to<IN> two<CD> electrons<NNS>']"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl_tag_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "aD3U73dG0ju4"
   },
   "outputs": [],
   "source": [
    "flair_res = []\n",
    "for tag_sent in fl_tag_sents:\n",
    "    list_t_s = tag_sent.split(' ')\n",
    "    reslist = [el.split('<')[0] + '<' + get_tag(re.search('<([^>]+)>', el).group(1), td_eng) + '>' for el in list_t_s]\n",
    "    flair_res += reslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8yX_ZhMU7yb"
   },
   "source": [
    "оценка accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7Wbm9ouO1n9S",
    "outputId": "d91c2422-f492-4eb9-efea-d4e3aed36ae3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alas<INTJ>',\n",
       " 'Oh<INTJ>',\n",
       " 'ah<INTJ>',\n",
       " 'It<PRON>',\n",
       " 'is<VERB>',\n",
       " 'ill<ADJ>',\n",
       " 'dealing<VERB>',\n",
       " 'with<PREP>',\n",
       " 'such<PRON>',\n",
       " 'a<ART>',\n",
       " 'foe<NOUN>',\n",
       " 'he<PRON>',\n",
       " 'is<VERB>',\n",
       " 'a<ART>',\n",
       " 'wizard<NOUN>',\n",
       " 'both<CONJ>',\n",
       " 'cunning<VERB>',\n",
       " 'and<CONJ>',\n",
       " 'dwimmer-crafty<ADJ>',\n",
       " 'having<VERB>',\n",
       " 'many<ADJ>',\n",
       " 'disguises<NOUN>',\n",
       " 'For<PREP>',\n",
       " 'instance<NOUN>',\n",
       " 'S9<NOUN>',\n",
       " 'subshells<NOUN>',\n",
       " 'have<VERB>',\n",
       " 'a<ART>',\n",
       " 'single<ADJ>',\n",
       " 'spherical<ADJ>',\n",
       " 'orbital<NOUN>',\n",
       " 'while<PREP>',\n",
       " 'A<NOUN>',\n",
       " 'B<NOUN>',\n",
       " 'Z<NOUN>',\n",
       " 'etc<FW>',\n",
       " 'subshells<NOUN>',\n",
       " 'contain<VERB>',\n",
       " 'three<NUMB>',\n",
       " 'dumbbell-shaped<VERB>',\n",
       " 'orbitals<NOUN>',\n",
       " 'at<PREP>',\n",
       " 'right<ADJ>',\n",
       " 'angles<NOUN>',\n",
       " 'to<PREP>',\n",
       " 'each<ART>',\n",
       " 'other<ADJ>',\n",
       " 'The<ART>',\n",
       " 'Bohr<NOUN>',\n",
       " 'model<NOUN>',\n",
       " 'is<VERB>',\n",
       " 'useful<ADJ>',\n",
       " 'to<PART>',\n",
       " 'explain<VERB>',\n",
       " 'the<ART>',\n",
       " 'reactivity<NOUN>',\n",
       " 'and<CONJ>',\n",
       " 'chemical<NOUN>',\n",
       " 'bonding<NOUN>',\n",
       " 'of<PREP>',\n",
       " 'many<ADJ>',\n",
       " 'elements<NOUN>',\n",
       " 'but<CONJ>',\n",
       " 'it<PRON>',\n",
       " 'actually<ADV>',\n",
       " 'does<VERB>',\n",
       " 'n’t<ADV>',\n",
       " 'give<VERB>',\n",
       " 'a<ART>',\n",
       " 'very<ADV>',\n",
       " 'accurate<ADJ>',\n",
       " 'description<NOUN>',\n",
       " 'of<PREP>',\n",
       " 'how<ADV>',\n",
       " 'electrons<NOUN>',\n",
       " 'are<VERB>',\n",
       " 'distributed<VERB>',\n",
       " 'in<PREP>',\n",
       " 'space<NOUN>',\n",
       " 'around<PREP>',\n",
       " 'the<ART>',\n",
       " 'nucleus<NOUN>',\n",
       " 'Specifically<ADV>',\n",
       " 'electrons<NOUN>',\n",
       " 'spend<VERB>',\n",
       " 'most<ADJ>',\n",
       " 'of<PREP>',\n",
       " 'their<PRON>',\n",
       " 'time<NOUN>',\n",
       " 'in<PREP>',\n",
       " 'sometimes-complex-shaped<ADJ>',\n",
       " 'regions<NOUN>',\n",
       " 'of<PREP>',\n",
       " 'space<NOUN>',\n",
       " 'around<PREP>',\n",
       " 'the<ART>',\n",
       " 'nucleus<NOUN>',\n",
       " 'known<VERB>',\n",
       " 'as<PREP>',\n",
       " 'electron<NOUN>',\n",
       " 'orbitals<NOUN>',\n",
       " 'This<ART>',\n",
       " 'high-probability<ADJ>',\n",
       " 'region<NOUN>',\n",
       " 'makes<VERB>',\n",
       " 'up<PART>',\n",
       " 'an<ART>',\n",
       " 'orbital<ADJ>',\n",
       " 'and<CONJ>',\n",
       " 'each<ART>',\n",
       " 'orbital<NOUN>',\n",
       " 'can<VERB>',\n",
       " 'hold<VERB>',\n",
       " 'up<PART>',\n",
       " 'to<PREP>',\n",
       " 'two<NUMB>',\n",
       " 'electrons<NOUN>']"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flair_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "3SYnsQfcU7yb",
    "outputId": "dbabb476-6dff-407a-e652-ad9eb7abd8ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7949\n"
     ]
    }
   ],
   "source": [
    "accuracy_check(flair_res, eng_text_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_iZUNGpU7ye"
   },
   "source": [
    "## лучший результат для русского показал теггер NATASHA\n",
    "выделение 3х синтакс групп\n",
    "\n",
    "не + ADJ\n",
    "\n",
    "не + VERB\n",
    "\n",
    "не + ADV\n",
    "\n",
    "я взяла все с \"не\", потому что оно меняет смысл на противоположный и это важно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13A2FRuHU7ym"
   },
   "source": [
    "взять лучший теггер для русского языка и с его помощью написать ф-ю \n",
    "котороая выделит 3 вида синтаксических групп (к примеру не + какая-то часть речи или NP или сущ.+ наречие или еще что-то), запись которых в словарь, по вашему мнению, улучшила бы качество работы программы и создать такую функцию или функции, которые с помощью любых известных нам средств (chunking и regexp grammar, Natasha syntax parser,  код с последнего занятия по SpyCy, etc.) будет выделять эти группы в поданном в нее тексте. \n",
    "2 балла за саму функцию, 1 балл за объяснение того, почему именно эти группы вы взяли.\n",
    "2 бонусных балла, если встроите эту функцию в программу из предыдущей домашки и сравните качества работы программы с нею и без неё."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "0boM1N67_jU7"
   },
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "8-MqjWX_U7yg"
   },
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "HQpVQG7Ci3i8"
   },
   "outputs": [],
   "source": [
    "def find_n_gramms(text):\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    natasha1 = [_.text + '<' + get_tag(_.pos) + '>' for _ in doc.tokens if _.text not in punct]\n",
    "    ne_s = []\n",
    "    for i in range(len(natasha1)):\n",
    "        ne = ''\n",
    "        txt = natasha1[i].split('<')[0]\n",
    "        if txt.lower() == 'не' and i != len(natasha1)-1:\n",
    "            ne = txt\n",
    "            next_token = natasha1[i+1].split('<')\n",
    "            if next_token[1].strip('>') in ['ADV', 'VERB', 'ADJ']:\n",
    "                ne_s.append(ne + ' ' + next_token[0])\n",
    "    return ne_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "NddjYsll-YGN",
    "outputId": "c6682124-e023-4af1-8aa0-ad511100069b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['не красиво', 'не жить']"
      ]
     },
     "execution_count": 98,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_n_gramms(\"Это было не красиво не.\\n КАк НЕ жить,,\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vY5mYs6QU7ym"
   },
   "source": [
    "### встроить ф-ю в прогу из предыдущ домашки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdO2wnxU_7Yl"
   },
   "source": [
    "**см. второй файл в репозитории**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5RpHeTXAAS7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "dz2_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
